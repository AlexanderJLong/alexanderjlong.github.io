---
---

@string{aps = {American Physical Society,}}

@inproceedings{long2022retrieval,
  abbr={CVPR},
  title={Retrieval Augmented Classification for Long-Tail Visual Recognition},
  author={Long, Alexander and Yin, Wei and Ajanthan, Thalaiyasingam and Nguyen, Vu and Purkait, Pulak and Garg, Ravi and Blair, Alan and Shen, Chunhua and van den Hengel, Anton},
  booktitle={Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition},
  pages={6959--6969},
  year={2022},
  preview={RAC.png},
  selected={true},
  bibtex_show={true},
  abstract={We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classification pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric external memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail classification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external information source. We demonstrate that RAC's retrieval module, without prompting, learns a high level of accuracy on tail classes. This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon. RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures.},
  url={https://arxiv.org/abs/2202.11233}

}

@inproceedings{long2023modality,
  abbr={ICLR 2023 (ME-FoMO)},
  title={Modality-Aware Adaptation of Contrastive Language-Image Models},
  author={Long, Alexander and Ajanthan, Thalaiyasingam and van den Hengel, Anton},
  booktitle={ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year={2023},
  selected={true},
  preview={modgap.png},
  bibtex_show={true},
  url={https://openreview.net/forum?id=KZut5N7waZ},
  abstract={Despite their high levels of robustness, Contrastive Language-Image Models (CLIP) still require some form of downstream adaptation when applied to tasks sufficiently out-of-domain with respect to their training set. Recent methods propose light-weight adapters on the model features, primarily focused on the few-shot domain. All such approaches however, require per-task hyperparameter tuning which necessitates access to a validation set; limiting their applicability in practice. As an alternative, we propose Modality Aware Tangent-space Retrieval (MATeR), a training-free, interpretable adapter which outperforms all recent methods when per-task hyperparameter tuning is prohibited. MATeR considers the manifold formed by CLIP embeddings when incorporating out of domain few-shot class information and its predictions are invariant to the modality gap; representing the first approach that considers the geometric structure of the CLIP latent space to inform downstream task adaptation. Additionally, we demonstrate a variant of MATeR has the ability to significantly increase zero-shot accuracy with only a handful of unlabelled images, much lower than the number of classes.}
}


@article{kaboli2015humanoids,
  title={Humanoids learn touch modalities identification via multi-modal robotic skin and robust tactile descriptors},
  author={Kaboli, Mohsen and Long, Alexander and Cheng, Gordon},
  journal={Advanced Robotics},
  volume={29},
  number={21},
  pages={1411--1425},
  year={2015},
  publisher={Taylor \& Francis},
  bibtex_show={true}
}

@inproceedings{long2022fast,
  abbr={AAAI 2022},
  title={Fast and Data Efficient Reinforcement Learning from Pixels via Non-parametric Value Approximation},
  author={Long, Alexander and Blair, Alan and van Hoof, Herke},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={7},
  pages={7620--7627},
  year={2022},
  bibtex_show={true}
}

@article{long2019multi,
  title={Multi-hop reading comprehension via deep reinforcement learning based document traversal},
  author={Long, Alex and Mason, Joel and Blair, Alan and Wang, Wei},
  journal={arXiv preprint arXiv:1905.09438},
  year={2019},
  bibtex_show={true}
}

@inproceedings{silva2024lipat,
  abbr={WACV 2024},
  title={LipAT: Beyond style transfer for controllable neural simulation of Lipstick using cosmetic attributes},
  author={Silva, Amila and Moskvyak, Olga and Long, Alexander and Garg, Ravi and Gould, Stephen and Avraham, Gil and van den Hengel, Anton},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={8046--8055},
  year={2024},
  bibtex_show={true}
}

@phdthesis{long2022external,
  title={External Nonparametric Memory in Deep Learning},
  author={Long, Alexander},
  year={2022},
  school={UNSW Sydney},
  bibtex_show={true}
}

